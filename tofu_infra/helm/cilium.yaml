# upgradeCompatibility helps users upgrading to ensure that the configMap for
# Cilium will not change critical values to ensure continued operation
# This flag is not required for new installations.
# For example: 1.7, 1.8, 1.9
upgradeCompatibility: '1.14'

debug:
  # -- Enable debug logging
  enabled: false
  # -- Configure verbosity levels for debug logging
  # This option is used to enable debug messages for operations related to such
  # sub-system such as (e.g. kvstore, envoy, datapath or policy), and flow is
  # for enabling debug messages emitted per request, message and connection.
  # Multiple values can be set via a space-separated string (e.g. "datapath envoy").
  #
  # Applicable values:
  # - flow
  # - kvstore
  # - envoy
  # - datapath
  # - policy
  verbose: ~

rbac:
  # -- Enable creation of Resource-Based Access Control configuration.
  create: true

# -- Configure image pull secrets for pulling container images
imagePullSecrets:
# - name: "image-pull-secret"

# -- (string) Kubernetes config path
# @default -- `"~/.kube/config"`
kubeConfigPath: ""
# -- (string) Kubernetes service host
k8sServiceHost: "${cluster_address}"
# -- (string) Kubernetes service port
k8sServicePort: 6443

# -- Agent container image.
image:
  override: ~
  repository: "quay.io/cilium/cilium"
  tag: "v1.15.3"
  pullPolicy: "IfNotPresent"
  # cilium-digest
  digest: "sha256:da74ab61d1bc665c1c088dff41d5be388d252ca5800f30c7d88844e6b5e440b0"
  useDigest: true

# -- Enables the fallback compatibility solution for when the xt_socket kernel
# module is missing and it is needed for the datapath L7 redirection to work
# properly. See documentation for details on when this can be disabled:
# https://docs.cilium.io/en/stable/operations/system_requirements/#linux-kernel.
enableXTSocketFallback: true

encryption:
  # -- Enable transparent network encryption.
  enabled: true

  # -- Enable Node encryption
  nodeEncryption: true

  # -- Encryption method. Can be either ipsec or wireguard.
  type: wireguard

  wireguard:
    # -- Enables the fallback to the user-space implementation.
    userspaceFallback: true
    # -- Controls Wireguard PersistentKeepalive option. Set 0s to disable.
    persistentKeepalive: 0s


endpointHealthChecking:
  # -- Enable connectivity health checking between virtual endpoints.
  enabled: true

# -- Enable endpoint status.
# Status can be: policy, health, controllers, log and / or state. For 2 or more options use a space.
endpointStatus:
  enabled: false
  status: ""

endpointRoutes:
  # -- Enable use of per endpoint routes instead of routing via
  # the cilium_host interface.
  enabled: false

k8sNetworkPolicy:
  # -- Enable support for K8s NetworkPolicy
  enabled: true

eni:
  # -- Enable Elastic Network Interface (ENI) integration.
  enabled: false
  # -- Update ENI Adapter limits from the EC2 API
  updateEC2AdapterLimitViaAPI: true
  # -- Release IPs not used from the ENI
  awsReleaseExcessIPs: false
  # -- Enable ENI prefix delegation
  awsEnablePrefixDelegation: false
  # -- EC2 API endpoint to use
  ec2APIEndpoint: ""
  # -- Tags to apply to the newly created ENIs
  eniTags: {}
  # -- Interval for garbage collection of unattached ENIs. Set to "0s" to disable.
  # @default -- `"5m"`
  gcInterval: ""
  # -- Additional tags attached to ENIs created by Cilium.
  # Dangling ENIs with this tag will be garbage collected
  # @default -- `{"io.cilium/cilium-managed":"true,"io.cilium/cluster-name":"<auto-detected>"}`
  gcTags: {}
  # -- If using IAM role for Service Accounts will not try to
  # inject identity values from cilium-aws kubernetes secret.
  # Adds annotation to service account if managed by Helm.
  # See https://github.com/aws/amazon-eks-pod-identity-webhook
  iamRole: ""
  # -- Filter via subnet IDs which will dictate which subnets are going to be used to create new ENIs
  # Important note: This requires that each instance has an ENI with a matching subnet attached
  # when Cilium is deployed. If you only want to control subnets for ENIs attached by Cilium,
  # use the CNI configuration file settings (cni.customConf) instead.
  subnetIDsFilter: []
  # -- Filter via tags (k=v) which will dictate which subnets are going to be used to create new ENIs
  # Important note: This requires that each instance has an ENI with a matching subnet attached
  # when Cilium is deployed. If you only want to control subnets for ENIs attached by Cilium,
  # use the CNI configuration file settings (cni.customConf) instead.
  subnetTagsFilter: []
  # -- Filter via AWS EC2 Instance tags (k=v) which will dictate which AWS EC2 Instances
  # are going to be used to create new ENIs
  instanceTagsFilter: []

externalIPs:
  # -- Enable ExternalIPs service support.
  enabled: true

# fragmentTracking enables IPv4 fragment tracking support in the datapath.
# fragmentTracking: true

gke:
  # -- Enable Google Kubernetes Engine integration
  enabled: false

# -- Enable connectivity health checking.
healthChecking: true

# -- TCP port for the agent health API. This is not the port for cilium-health.
healthPort: 9879

# -- Configure the host firewall.
hostFirewall:
  # -- Enables the enforcement of host policies in the eBPF datapath.
  enabled: false

hostPort:
  # -- Enable hostPort service support.
  enabled: true

# -- Configure socket LB
socketLB:
  # -- Enable socket LB
  enabled: false

  # -- Disable socket lb for non-root ns. This is used to enable Istio routing rules.
  # hostNamespaceOnly: false

# -- Configure certificate generation for Hubble integration.
# If hubble.tls.auto.method=cronJob, these values are used
# for the Kubernetes CronJob which will be scheduled regularly to
# (re)generate any certificates not provided manually.
certgen:
  image:
    override: ~
    repository: "quay.io/cilium/certgen"
    tag: "v0.1.9"
    digest: "sha256:89a0847753686444daabde9474b48340993bd19c7bea66a46e45b2974b82041f"
    useDigest: true
    pullPolicy: "IfNotPresent"
  # -- Seconds after which the completed job pod will be deleted
  ttlSecondsAfterFinished: 1800
  # -- Labels to be added to hubble-certgen pods
  podLabels: {}
  # -- Annotations to be added to the hubble-certgen initial Job and CronJob
  annotations:
    job: {}
    cronJob: {}
  # -- Node tolerations for pod assignment on nodes with taints
  # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations: []

  # -- Additional certgen volumes.
  extraVolumes: []

  # -- Additional certgen volumeMounts.
  extraVolumeMounts: []

  # -- Affinity for certgen
  affinity: {}

hubble:
  # -- Enable Hubble (true by default).
  enabled: true

  # -- Annotations to be added to all top-level hubble objects (resources under templates/hubble)
  annotations: {}

  # -- Buffer size of the channel Hubble uses to receive monitor events. If this
  # value is not set, the queue size is set to the default monitor queue size.
  # eventQueueSize: ""

  # -- Number of recent flows for Hubble to cache. Defaults to 4095.
  # Possible values are:
  #   1, 3, 7, 15, 31, 63, 127, 255, 511, 1023,
  #   2047, 4095, 8191, 16383, 32767, 65535
  # eventBufferCapacity: "4095"

  # -- Hubble metrics configuration.
  # See https://docs.cilium.io/en/stable/observability/metrics/#hubble-metrics
  # for more comprehensive documentation about Hubble metrics.
  metrics:
    # -- Configures the list of metrics to collect. If empty or null, metrics
    # are disabled.
    # Example:
    #
    #   enabled:
    #   - dns:query;ignoreAAAA
    #   - drop
    #   - tcp
    #   - flow
    #   - icmp
    #   - http
    #
    # You can specify the list of metrics from the helm CLI:
    #
    #   --set hubble.metrics.enabled="{dns:query;ignoreAAAA,drop,tcp,flow,icmp,http}"
    #
    enabled:
      - dns:query;ignoreAAAA
      - drop
      - tcp
      - flow
      - icmp
      - http
    # -- Configure the port the hubble metric server listens on.
    port: 9965
    # -- Annotations to be added to hubble-metrics service.
    serviceAnnotations: {}
    serviceMonitor:
      # -- Create ServiceMonitor resources for Prometheus Operator.
      # This requires the prometheus CRDs to be available.
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
      enabled: false
      # -- Labels to add to ServiceMonitor hubble
      labels: {}
      # -- Annotations to add to ServiceMonitor hubble
      annotations: {}
      # -- jobLabel to add for ServiceMonitor hubble
      jobLabel: ""
      # -- Interval for scrape metrics.
      interval: "10s"
      # -- Relabeling configs for the ServiceMonitor hubble
      relabelings:
        - sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: node
          replacement: ${1}
      # -- Metrics relabeling configs for the ServiceMonitor hubble
      metricRelabelings: ~
    # -- Grafana dashboards for hubble
    # grafana can import dashboards based on the label and value
    # ref: https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-dashboards
    dashboards:
      enabled: false
      label: grafana_dashboard
      namespace: clops
      labelValue: "1"
      annotations: {}

  # -- Unix domain socket path to listen to when Hubble is enabled.
  socketPath: /var/run/cilium/hubble.sock

  # -- An additional address for Hubble to listen to.
  # Set this field ":4244" if you are enabling Hubble Relay, as it assumes that
  # Hubble is listening on port 4244.
  listenAddress: ":4244"
  # -- Whether Hubble should prefer to announce IPv6 or IPv4 addresses if both are available.
  preferIpv6: false
  # -- (bool) Skip Hubble events with unknown cgroup ids
  # @default -- `true`
  skipUnknownCGroupIDs: ~

  peerService:
    # -- Service Port for the Peer service.
    # If not set, it is dynamically assigned to port 443 if TLS is enabled and to
    # port 80 if not.
    # servicePort: 80
    # -- Target Port for the Peer service, must match the hubble.listenAddress'
    # port.
    targetPort: 4244
    # -- The cluster domain to use to query the Hubble Peer service. It should
    # be the local cluster.
    clusterDomain: cluster.local
  # -- TLS configuration for Hubble
  tls:
    # -- Enable mutual TLS for listenAddress. Setting this value to false is
    # highly discouraged as the Hubble API provides access to potentially
    # sensitive network flow metadata and is exposed on the host network.
    enabled: true
    # -- Configure automatic TLS certificates generation.
    auto:
      # -- Auto-generate certificates.
      # When set to true, automatically generate a CA and certificates to
      # enable mTLS between Hubble server and Hubble Relay instances. If set to
      # false, the certs for Hubble server need to be provided by setting
      # appropriate values below.
      enabled: true
      # -- Set the method to auto-generate certificates. Supported values:
      # - helm:         This method uses Helm to generate all certificates.
      # - cronJob:      This method uses a Kubernetes CronJob the generate any
      #                 certificates not provided by the user at installation
      #                 time.
      # - certmanager:  This method use cert-manager to generate & rotate certificates.
      method: helm
      # -- Generated certificates validity duration in days.
      certValidityDuration: 1095
      # -- Schedule for certificates regeneration (regardless of their expiration date).
      # Only used if method is "cronJob". If nil, then no recurring job will be created.
      # Instead, only the one-shot job is deployed to generate the certificates at
      # installation time.
      #
      # Defaults to midnight of the first day of every fourth month. For syntax, see
      # https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#schedule-syntax
      schedule: "0 0 1 */4 *"

      # [Example]
      # certManagerIssuerRef:
      #   group: cert-manager.io
      #   kind: ClusterIssuer
      #   name: ca-issuer
      # -- certmanager issuer used when hubble.tls.auto.method=certmanager.
      certManagerIssuerRef: {}

    # -- base64 encoded PEM values for the Hubble server certificate and private key
    server:
      cert: ""
      key: ""
      # -- Extra DNS names added to certificate when it's auto generated
      extraDnsNames: []
      # -- Extra IP addresses added to certificate when it's auto generated
      extraIpAddresses: []

  relay:
    # -- Enable Hubble Relay (requires hubble.enabled=true)
    enabled: true

    # -- Roll out Hubble Relay pods automatically when configmap is updated.
    rollOutPods: true

    # -- Hubble-relay container image.
    image:
      override: ~
      repository: "quay.io/cilium/hubble-relay"
      tag: "v1.15.3"
       # hubble-relay-digest
      digest: "sha256:b9c6431aa4f22242a5d0d750c621d9d04bdc25549e4fb1116bfec98dd87958a2"
      useDigest: true
      pullPolicy: "IfNotPresent"

    # -- Specifies the resources for the hubble-relay pods
    resources: {}

    # -- Number of replicas run for the hubble-relay deployment.
    replicas: 1

    # -- Affinity for hubble-replay
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              k8s-app: cilium

    # -- Pod topology spread constraints for hubble-relay
    topologySpreadConstraints: []
      # - maxSkew: 1
      #   topologyKey: topology.kubernetes.io/zone
      #   whenUnsatisfiable: DoNotSchedule

    # -- Node labels for pod assignment
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    nodeSelector:
      kubernetes.io/os: linux

    # -- Node tolerations for pod assignment on nodes with taints
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    tolerations: []

    # -- Additional hubble-relay environment variables.
    extraEnv: []

    # -- Annotations to be added to all top-level hubble-relay objects (resources under templates/hubble-relay)
    annotations: {}

    # -- Annotations to be added to hubble-relay pods
    podAnnotations: {}

    # -- Labels to be added to hubble-relay pods
    podLabels: {}

    # PodDisruptionBudget settings
    podDisruptionBudget:
      # -- enable PodDisruptionBudget
      # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
      enabled: false
      # -- Minimum number/percentage of pods that should remain scheduled.
      # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
      minAvailable: null
      # -- Maximum number/percentage of pods that may be made unavailable
      maxUnavailable: 1

    # -- The priority class to use for hubble-relay
    priorityClassName: ""

    # -- Configure termination grace period for hubble relay Deployment.
    terminationGracePeriodSeconds: 1

    # -- hubble-relay update strategy
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        maxUnavailable: 1

    # -- Additional hubble-relay volumes.
    extraVolumes: []

    # -- Additional hubble-relay volumeMounts.
    extraVolumeMounts: []

    # -- hubble-relay pod security context
    podSecurityContext:
      fsGroup: 65532

    # -- hubble-relay container security context
    securityContext:
      # readOnlyRootFilesystem: true
      runAsNonRoot: true
      runAsUser: 65532
      runAsGroup: 65532
      capabilities:
        drop:
        - ALL

    # -- hubble-relay service configuration.
    service:
      # --- The type of service used for Hubble Relay access, either ClusterIP or NodePort.
      type: ClusterIP
      # --- The port to use when the service type is set to NodePort.
      nodePort: 31234

    # -- Host to listen to. Specify an empty string to bind to all the interfaces.
    listenHost: ""

    # -- Port to listen to.
    listenPort: "4245"

    # -- TLS configuration for Hubble Relay
    tls:
      # -- base64 encoded PEM values for the hubble-relay client certificate and private key
      # This keypair is presented to Hubble server instances for mTLS
      # authentication and is required when hubble.tls.enabled is true.
      # These values need to be set manually if hubble.tls.auto.enabled is false.
      client:
        cert: ""
        key: ""
      # -- base64 encoded PEM values for the hubble-relay server certificate and private key
      server:
        # When set to true, enable TLS on for Hubble Relay server
        # (ie: for clients connecting to the Hubble Relay API).
        enabled: false
        # When set to true enforces mutual TLS between Hubble Relay server and its clients.
        # False allow non-mutual TLS connections.
        # This option has no effect when TLS is disabled.
        mtls: false
        # These values need to be set manually if hubble.tls.auto.enabled is false.
        cert: ""
        key: ""
        # -- extra DNS names added to certificate when its auto gen
        extraDnsNames: []
        # -- extra IP addresses added to certificate when its auto gen
        extraIpAddresses: []
        # DNS name used by the backend to connect to the relay
        # This is a simple workaround as the relay certificates are currently hardcoded to 
        # *.hubble-relay.cilium.io 
        # See https://github.com/cilium/cilium/pull/28709#discussion_r1371792546
        # For GKE Dataplane V2 this should be set to relay.kube-system.svc.cluster.local
        relayName: "ui.hubble-relay.cilium.io"

    # -- Dial timeout to connect to the local hubble instance to receive peer information (e.g. "30s").
    dialTimeout: ~

    # -- Backoff duration to retry connecting to the local hubble instance in case of failure (e.g. "30s").
    retryTimeout: ~

    # -- Max number of flows that can be buffered for sorting before being sent to the
    # client (per request) (e.g. 100).
    sortBufferLenMax: ~

    # -- When the per-request flows sort buffer is not full, a flow is drained every
    # time this timeout is reached (only affects requests in follow-mode) (e.g. "1s").
    sortBufferDrainTimeout: ~

    # -- Port to use for the k8s service backed by hubble-relay pods.
    # If not set, it is dynamically assigned to port 443 if TLS is enabled and to
    # port 80 if not.
    # servicePort: 80

    # -- Enable prometheus metrics for hubble-relay on the configured port at
    # /metrics
    prometheus:
      enabled: false
      port: 9966
      serviceMonitor:
        # -- Enable service monitors.
        # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
        enabled: false
        # -- Labels to add to ServiceMonitor hubble-relay
        labels: {}
        # -- Annotations to add to ServiceMonitor hubble-relay
        annotations: {}
        # -- Interval for scrape metrics.
        interval: "10s"
        # -- Specify the Kubernetes namespace where Prometheus expects to find
        # service monitors configured.
        # namespace: ""
        # -- Relabeling configs for the ServiceMonitor hubble-relay
        relabelings: ~
        # -- Metrics relabeling configs for the ServiceMonitor hubble-relay
        metricRelabelings: ~

    gops:
      # -- Enable gops for hubble-relay
      enabled: true
      # -- Configure gops listen port for hubble-relay
      port: 9893

    pprof:
      # -- Enable pprof for hubble-relay
      enabled: false
      # -- Configure pprof listen address for hubble-relay
      address: localhost
      # -- Configure pprof listen port for hubble-relay
      port: 6062

  ui:
    # -- Whether to enable the Hubble UI.
    enabled: false

    standalone:
      # -- When true, it will allow installing the Hubble UI only, without checking dependencies.
      # It is useful if a cluster already has cilium and Hubble relay installed and you just
      # want Hubble UI to be deployed.
      # When installed via helm, installing UI should be done via `helm upgrade` and when installed via the cilium cli, then `cilium hubble enable --ui`
      enabled: false

      tls:
        # -- When deploying Hubble UI in standalone, with tls enabled for Hubble relay, it is required
        # to provide a volume for mounting the client certificates.
        certsVolume: {}
          # projected:
          #   defaultMode: 0400
          #   sources:
          #   - secret:
          #       name: hubble-ui-client-certs
          #       items:
          #       - key: tls.crt
          #         path: client.crt
          #       - key: tls.key
          #         path: client.key
          #       - key: ca.crt
          #         path: hubble-relay-ca.crt

    # -- Roll out Hubble-ui pods automatically when configmap is updated.
    rollOutPods: false

    tls:
      # -- base64 encoded PEM values used to connect to hubble-relay
      # This keypair is presented to Hubble Relay instances for mTLS
      # authentication and is required when hubble.relay.tls.server.enabled is true.
      # These values need to be set manually if hubble.tls.auto.enabled is false.
      client:
        cert: ""
        key: ""

    backend:
      # -- Hubble-ui backend image.
      image:
        override: ~
        repository: "quay.io/cilium/hubble-ui-backend"
        tag: "v0.13.0"
        digest: "sha256:1e7657d997c5a48253bb8dc91ecee75b63018d16ff5e5797e5af367336bc8803"
        useDigest: true
        pullPolicy: "IfNotPresent"

      # -- Hubble-ui backend security context.
      securityContext: {}

      # -- Additional hubble-ui backend environment variables.
      extraEnv: []

      # -- Additional hubble-ui backend volumes.
      extraVolumes: []

      # -- Additional hubble-ui backend volumeMounts.
      extraVolumeMounts: []

      livenessProbe:
        # -- Enable liveness probe for Hubble-ui backend (requires Hubble-ui 0.12+)
        enabled: false

      readinessProbe:
        # -- Enable readiness probe for Hubble-ui backend (requires Hubble-ui 0.12+)
        enabled: false

      # -- Resource requests and limits for the 'backend' container of the 'hubble-ui' deployment.
      resources: {}
      #   limits:
      #     cpu: 1000m
      #     memory: 1024M
      #   requests:
      #     cpu: 100m
      #     memory: 64Mi

    frontend:
      # -- Hubble-ui frontend image.
      image:
        override: ~
        repository: "quay.io/cilium/hubble-ui"
        tag: "v0.13.0"
        digest: "sha256:7d663dc16538dd6e29061abd1047013a645e6e69c115e008bee9ea9fef9a6666"
        useDigest: true
        pullPolicy: "IfNotPresent"

      # -- Hubble-ui frontend security context.
      securityContext: {}

      # -- Additional hubble-ui frontend environment variables.
      extraEnv: []

      # -- Additional hubble-ui frontend volumes.
      extraVolumes: []

      # -- Additional hubble-ui frontend volumeMounts.
      extraVolumeMounts: []

      # -- Resource requests and limits for the 'frontend' container of the 'hubble-ui' deployment.
      resources: {}
      #   limits:
      #     cpu: 1000m
      #     memory: 1024M
      #   requests:
      #     cpu: 100m
      #     memory: 64Mi
      server:
        # -- Controls server listener for ipv6
        ipv6:
          enabled: true

    # -- The number of replicas of Hubble UI to deploy.
    replicas: 1

    # -- Annotations to be added to all top-level hubble-ui objects (resources under templates/hubble-ui)
    annotations: {}

    # -- Annotations to be added to hubble-ui pods
    podAnnotations: {}

    # -- Labels to be added to hubble-ui pods
    podLabels: {}

    # PodDisruptionBudget settings
    podDisruptionBudget:
      # -- enable PodDisruptionBudget
      # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
      enabled: false
      # -- Minimum number/percentage of pods that should remain scheduled.
      # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
      minAvailable: null
      # -- Maximum number/percentage of pods that may be made unavailable
      maxUnavailable: 1

    # -- Affinity for hubble-ui
    affinity: {}

    # -- Pod topology spread constraints for hubble-ui
    topologySpreadConstraints: []
      # - maxSkew: 1
      #   topologyKey: topology.kubernetes.io/zone
      #   whenUnsatisfiable: DoNotSchedule

    # -- Node labels for pod assignment
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector
    nodeSelector:
      kubernetes.io/os: linux

    # -- Node tolerations for pod assignment on nodes with taints
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    tolerations: []

    # -- The priority class to use for hubble-ui
    priorityClassName: ""

    # -- hubble-ui update strategy.
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        maxUnavailable: 1

    # -- Security context to be added to Hubble UI pods
    securityContext:
      runAsUser: 1001
      runAsGroup: 1001
      fsGroup: 1001

    # -- hubble-ui service configuration.
    service:
      # -- Annotations to be added for the Hubble UI service
      annotations: {}
      # --- The type of service used for Hubble UI access, either ClusterIP or NodePort.
      type: ClusterIP
      # --- The port to use when the service type is set to NodePort.
      nodePort: 31235

    # -- Defines base url prefix for all hubble-ui http requests.
    # It needs to be changed in case if ingress for hubble-ui is configured under some sub-path.
    # Trailing `/` is required for custom path, ex. `/service-map/`
    baseUrl: "/"

    # -- hubble-ui ingress configuration.
    ingress:
      enabled: false
      annotations: {}
        # kubernetes.io/ingress.class: nginx
        # kubernetes.io/tls-acme: "true"
      className: ""
      hosts:
        - chart-example.local
      labels: {}
      tls: []
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local

  # -- Hubble flows export.
  export:
    # --- Defines max file size of output file before it gets rotated.
    fileMaxSizeMb: 10
    # --- Defines max number of backup/rotated files.
    fileMaxBackups: 5
    # --- Static exporter configuration.
    # Static exporter is bound to agent lifecycle.
    static:
      enabled: false
      filePath: /var/run/cilium/hubble/events.log
      fieldMask: []
      # - time
      # - source
      # - destination
      # - verdict
      allowList: []
      # - '{"verdict":["DROPPED","ERROR"]}'
      denyList: []
      # - '{"source_pod":["kube-system/"]}'
      # - '{"destination_pod":["kube-system/"]}'
    # --- Dynamic exporters configuration.
    # Dynamic exporters may be reconfigured without a need of agent restarts.
    dynamic:
      enabled: false
      config:
        # ---- Name of configmap with configuration that may be altered to reconfigure exporters within a running agents.
        configMapName: cilium-flowlog-config
        # ---- True if helm installer should create config map.
        # Switch to false if you want to self maintain the file content.
        createConfigMap: true
        # ---- Exporters configuration in YAML format.
        content:
        - name: all
          fieldMask: []
          includeFilters: []
          excludeFilters: []
          filePath: "/var/run/cilium/hubble/events.log"
        #- name: "test002"
        #  filePath: "/var/log/network/flow-log/pa/test002.log"
        #  fieldMask: ["source.namespace", "source.pod_name", "destination.namespace", "destination.pod_name", "verdict"]
        #  includeFilters:
        #  - source_pod: ["default/"]
        #    event_type:
        #    - type: 1
        #  - destination_pod: ["frontend/nginx-975996d4c-7hhgt"]
        #  excludeFilters: []
        #  end: "2023-10-09T23:59:59-07:00"

# -- Method to use for identity allocation (`crd` or `kvstore`).
identityAllocationMode: "crd"

# -- (string) Time to wait before using new identity on endpoint identity change.
# @default -- `"5s"`
identityChangeGracePeriod: ""

# -- Install Iptables rules to skip netfilter connection tracking on all pod
# traffic. This option is only effective when Cilium is running in direct
# routing and full KPR mode. Moreover, this option cannot be enabled when Cilium
# is running in a managed Kubernetes environment or in a chained CNI setup.
installNoConntrackIptablesRules: false

ipam:
  # -- Configure IP Address Management mode.
  # ref: https://docs.cilium.io/en/stable/network/concepts/ipam/
  mode: "cluster-pool"
  # -- Maximum rate at which the CiliumNode custom resource is updated.
  ciliumNodeUpdateRate: "15s"
  operator:
    # -- IPv4 CIDR list range to delegate to individual nodes for IPAM.
    clusterPoolIPv4PodCIDRList: ["10.0.0.0/8"]
    # -- IPv4 CIDR mask size to delegate to individual nodes for IPAM.
    clusterPoolIPv4MaskSize: 24
    # -- IPv6 CIDR list range to delegate to individual nodes for IPAM.
    clusterPoolIPv6PodCIDRList: ["fd00::/104"]
    # -- IPv6 CIDR mask size to delegate to individual nodes for IPAM.
    clusterPoolIPv6MaskSize: 120
    # -- IP pools to auto-create in multi-pool IPAM mode.
    autoCreateCiliumPodIPPools: {}
      # default:
      #   ipv4:
      #     cidrs:
      #       - 10.10.0.0/8
      #     maskSize: 24
      # other:
      #   ipv6:
      #     cidrs:
      #       - fd00:100::/80
      #     maskSize: 96
    # -- The maximum burst size when rate limiting access to external APIs.
    # Also known as the token bucket capacity.
    # @default -- `20`
    externalAPILimitBurstSize: ~
    # -- The maximum queries per second when rate limiting access to
    # external APIs. Also known as the bucket refill rate, which is used to
    # refill the bucket up to the burst size capacity.
    # @default -- `4.0`
    externalAPILimitQPS: ~

# -- The api-rate-limit option can be used to overwrite individual settings of the default configuration for rate limiting calls to the Cilium Agent API
apiRateLimit: ~

# -- Configure the eBPF-based ip-masq-agent
ipMasqAgent:
  enabled: false
# the config of nonMasqueradeCIDRs
# config:
  # nonMasqueradeCIDRs: []
  # masqLinkLocal: false
  # masqLinkLocalIPv6: false

# iptablesLockTimeout defines the iptables "--wait" option when invoked from Cilium.
# iptablesLockTimeout: "5s"

ipv4:
  # -- Enable IPv4 support.
  enabled: true

ipv6:
  # -- Enable IPv6 support.
  enabled: false

# -- Configure Kubernetes specific configuration
k8s: {}
  # -- requireIPv4PodCIDR enables waiting for Kubernetes to provide the PodCIDR
  # range via the Kubernetes node resource
  # requireIPv4PodCIDR: false

  # -- requireIPv6PodCIDR enables waiting for Kubernetes to provide the PodCIDR
  # range via the Kubernetes node resource
  # requireIPv6PodCIDR: false

# -- Keep the deprecated selector labels when deploying Cilium DaemonSet.
keepDeprecatedLabels: false

# -- Keep the deprecated probes when deploying Cilium DaemonSet
keepDeprecatedProbes: false

startupProbe:
  # -- failure threshold of startup probe.
  # 105 x 2s translates to the old behaviour of the readiness probe (120s delay + 30 x 3s)
  failureThreshold: 105
  # -- interval between checks of the startup probe
  periodSeconds: 2
livenessProbe:
  # -- failure threshold of liveness probe
  failureThreshold: 10
  # -- interval between checks of the liveness probe
  periodSeconds: 30
readinessProbe:
  # -- failure threshold of readiness probe
  failureThreshold: 3
  # -- interval between checks of the readiness probe
  periodSeconds: 30

# -- Configure the kube-proxy replacement in Cilium BPF datapath
# Valid options are "true", "false", "disabled" (deprecated), "partial" (deprecated), "strict" (deprecated).
# ref: https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/
kubeProxyReplacement: "true"

# -- healthz server bind address for the kube-proxy replacement.
# To enable set the value to '0.0.0.0:10256' for all ipv4
# addresses and this '[::]:10256' for all ipv6 addresses.
# By default it is disabled.
kubeProxyReplacementHealthzBindAddr: ""

l2NeighDiscovery:
  # -- Enable L2 neighbor discovery in the agent
  enabled: true
  # -- Override the agent's default neighbor resolution refresh period.
  refreshPeriod: "30s"

# -- Enable Layer 7 network policy.
l7Proxy: false

# -- Enable Local Redirect Policy.
localRedirectPolicy: false

# To include or exclude matched resources from cilium identity evaluation
# labels: ""

# logOptions allows you to define logging options. eg:
# logOptions:
#   format: json

# -- Enables periodic logging of system load
logSystemLoad: false


# -- Configure maglev consistent hashing
maglev: {}
  # -- tableSize is the size (parameter M) for the backend table of one
  # service entry
  # tableSize:

  # -- hashSeed is the cluster-wide base64 encoded seed for the hashing
  # hashSeed:

# -- Enables masquerading of IPv4 traffic leaving the node from endpoints.
enableIPv4Masquerade: true

# -- Enables masquerading of IPv6 traffic leaving the node from endpoints.
enableIPv6Masquerade: true

# -- Enables masquerading to the source of the route for traffic leaving the node from endpoints.
enableMasqueradeRouteSource: false

# -- Enables IPv4 BIG TCP support which increases maximum IPv4 GSO/GRO limits for nodes and pods
enableIPv4BIGTCP: false

# -- Enables IPv6 BIG TCP support which increases maximum IPv6 GSO/GRO limits for nodes and pods
enableIPv6BIGTCP: false

egressGateway:
  # -- Enables egress gateway to redirect and SNAT the traffic that leaves the
  # cluster.
  enabled: false
  # -- Deprecated without a replacement necessary.
  installRoutes: false
  # -- Time between triggers of egress gateway state reconciliations
  reconciliationTriggerInterval: 1s
  # -- Maximum number of entries in egress gateway policy map
  # maxPolicyEntries: 16384

vtep:
# -- Enables VXLAN Tunnel Endpoint (VTEP) Integration (beta) to allow
# Cilium-managed pods to talk to third party VTEP devices over Cilium tunnel.
  enabled: false

# -- A space separated list of VTEP device endpoint IPs, for example "1.1.1.1  1.1.2.1"
  endpoint: ""
# -- A space separated list of VTEP device CIDRs, for example "1.1.1.0/24 1.1.2.0/24"
  cidr: ""
# -- VTEP CIDRs Mask that applies to all VTEP CIDRs, for example "255.255.255.0"
  mask: ""
# -- A space separated list of VTEP device MAC addresses (VTEP MAC), for example "x:x:x:x:x:x  y:y:y:y:y:y:y"
  mac: ""

# -- (string) Allows to explicitly specify the IPv4 CIDR for native routing.
# When specified, Cilium assumes networking for this CIDR is preconfigured and
# hands traffic destined for that range to the Linux network stack without
# applying any SNAT.
# Generally speaking, specifying a native routing CIDR implies that Cilium can
# depend on the underlying networking stack to route packets to their
# destination. To offer a concrete example, if Cilium is configured to use
# direct routing and the Kubernetes CIDR is included in the native routing CIDR,
# the user must configure the routes to reach pods, either manually or by
# setting the auto-direct-node-routes flag.
ipv4NativeRoutingCIDR: ""

# -- (string) Allows to explicitly specify the IPv6 CIDR for native routing.
# When specified, Cilium assumes networking for this CIDR is preconfigured and
# hands traffic destined for that range to the Linux network stack without
# applying any SNAT.
# Generally speaking, specifying a native routing CIDR implies that Cilium can
# depend on the underlying networking stack to route packets to their
# destination. To offer a concrete example, if Cilium is configured to use
# direct routing and the Kubernetes CIDR is included in the native routing CIDR,
# the user must configure the routes to reach pods, either manually or by
# setting the auto-direct-node-routes flag.
ipv6NativeRoutingCIDR: ""

# -- cilium-monitor sidecar.
monitor:
  # -- Enable the cilium-monitor sidecar.
  enabled: false

# -- Configure service load balancing
loadBalancer:
  # -- standalone enables the standalone L4LB which does not connect to
  # kube-apiserver.
  # standalone: false

  # -- algorithm is the name of the load balancing algorithm for backend
  # selection e.g. random or maglev
  # algorithm: random

  # -- mode is the operation mode of load balancing for remote backends
  # e.g. snat, dsr, hybrid
  # mode: snat

  # -- acceleration is the option to accelerate service handling via XDP
  # Applicable values can be: disabled (do not use XDP), native (XDP BPF
  # program is run directly out of the networking driver's early receive
  # path), or best-effort (use native mode XDP acceleration on devices
  # that support it).
  acceleration: disabled

  # -- dsrDispatch configures whether IP option or IPIP encapsulation is
  # used to pass a service IP and port to remote backend
  # dsrDispatch: opt

  # -- serviceTopology enables K8s Topology Aware Hints -based service
  # endpoints filtering
  # serviceTopology: false

  # -- L7 LoadBalancer
  l7:
    # -- Enable L7 service load balancing via envoy proxy.
    # The request to a k8s service, which has specific annotation e.g. service.cilium.io/lb-l7,
    # will be forwarded to the local backend proxy to be load balanced to the service endpoints.
    # Please refer to docs for supported annotations for more configuration.
    #
    # Applicable values:
    #   - envoy: Enable L7 load balancing via envoy proxy. This will automatically set enable-envoy-config as well.
    #   - disabled: Disable L7 load balancing by way of service annotation.
    backend: disabled
    # -- List of ports from service to be automatically redirected to above backend.
    # Any service exposing one of these ports will be automatically redirected.
    # Fine-grained control can be achieved by using the service annotation.
    ports: []
    # -- Default LB algorithm
    # The default LB algorithm to be used for services, which can be overridden by the
    # service annotation (e.g. service.cilium.io/lb-l7-algorithm)
    # Applicable values: round_robin, least_request, random
    algorithm: round_robin

# -- Configure N-S k8s service loadbalancing
nodePort:
  # -- Enable the Cilium NodePort service implementation.
  enabled: true

  # -- Port range to use for NodePort services.
  # range: "30000,32767"

  # -- Set to true to prevent applications binding to service ports.
  bindProtection: true

  # -- Append NodePort range to ip_local_reserved_ports if clash with ephemeral
  # ports is detected.
  autoProtectPortRange: true

  # -- Enable healthcheck nodePort server for NodePort services
  enableHealthCheck: true

# -- Configure TLS configuration in the agent.
tls:
  # -- This configures how the Cilium agent loads the secrets used TLS-aware CiliumNetworkPolicies
  # (namely the secrets referenced by terminatingTLS and originatingTLS).
  # Possible values:
  #   - local
  #   - k8s
  secretsBackend: local

preflight:
  # -- Enable Cilium pre-flight resources (required for upgrade)
  enabled: false
